{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70f2bec6",
   "metadata": {},
   "source": [
    "# Fine-Tuning BERT model (reference : [Hugging Face](https://huggingface.co/docs/transformers/en/model_doc/bert))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe488d76",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sanat_14/miniconda3/envs/bert_pytorch/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392e0eb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 35020/35020 [00:05<00:00, 6175.21 examples/s]\n",
      "Map: 100%|██████████| 7480/7480 [00:01<00:00, 7410.24 examples/s]\n",
      "Map: 100%|██████████| 7500/7500 [00:01<00:00, 6098.33 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess data\n",
    "df = pd.read_csv(\"../data/clean/cleaned_imdb_data.csv\")\n",
    "df['sentiment'] = df['sentiment'].map({'negative': 0, 'positive': 1})\n",
    "df = df[['review', 'sentiment']].rename(columns={'review': 'text', 'sentiment': 'label'})\n",
    "\n",
    "train_val, test = train_test_split(df, test_size=0.15, random_state=42, stratify=df['label'])\n",
    "train, val = train_test_split(train_val, test_size=0.176, random_state=42, stratify=train_val['label'])\n",
    "\n",
    "# Convert to Hugging Face datasets\n",
    "train_dataset = Dataset.from_pandas(train)\n",
    "val_dataset = Dataset.from_pandas(val)\n",
    "test_dataset = Dataset.from_pandas(test)\n",
    "\n",
    "# Tokenization using BERT tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "def tokenize_fn(batch):\n",
    "    return tokenizer(batch[\"text\"], padding=\"max_length\", truncation=True, max_length=256)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_fn, batched=True)\n",
    "val_dataset = val_dataset.map(tokenize_fn, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize_fn, batched=True)\n",
    "\n",
    "# Clean and format datasets\n",
    "for ds in [train_dataset, val_dataset, test_dataset]:\n",
    "    ds = ds.remove_columns([col for col in [\"text\", \"__index_level_0__\"] if col in ds.column_names])\n",
    "    ds.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "679f5b59",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_515/1520731972.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Load pretrained BERT model\n",
    "model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=2)\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert_sentiment\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    fp16=True,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=50,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    save_total_limit=1,\n",
    ")\n",
    "\n",
    "# Define metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    preds = np.argmax(logits, axis=-1)\n",
    "    return {\"accuracy\": accuracy_score(labels, preds)}\n",
    "\n",
    "# Initialize Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5f8462a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6567' max='6567' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6567/6567 32:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.313000</td>\n",
       "      <td>0.271815</td>\n",
       "      <td>0.902005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.272562</td>\n",
       "      <td>0.911096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.170800</td>\n",
       "      <td>0.381633</td>\n",
       "      <td>0.911898</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='118' max='118' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [118/118 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Test Accuracy: 0.9141\n"
     ]
    }
   ],
   "source": [
    "# Train and evaluate\n",
    "trainer.train()\n",
    "results = trainer.evaluate(test_dataset)\n",
    "print(f\"Final Test Accuracy: {results['eval_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f5ec5471",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bert_sentiment_model/tokenizer_config.json',\n",
       " './bert_sentiment_model/special_tokens_map.json',\n",
       " './bert_sentiment_model/vocab.txt',\n",
       " './bert_sentiment_model/added_tokens.json',\n",
       " './bert_sentiment_model/tokenizer.json')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model and tokenizer\n",
    "model.save_pretrained(\"./bert_sentiment_model\")\n",
    "tokenizer.save_pretrained(\"./bert_sentiment_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763c302",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17eeffe8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_515/2909035532.py:9: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='938' max='938' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [938/938 00:33]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy : 0.9141\n",
      "Test Loss : 0.3739\n"
     ]
    }
   ],
   "source": [
    "model_path = \"./bert_sentiment_model\"\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "tokenizer = BertTokenizerFast.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Reload Trainer with the loaded model\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Evaluate on test dataset\n",
    "results = trainer.evaluate(test_dataset)\n",
    "\n",
    "print(f\"Test Accuracy : {results['eval_accuracy']:.4f}\")\n",
    "print(f\"Test Loss : {results['eval_loss']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "720fb7d6",
   "metadata": {},
   "source": [
    "#### Summary of Results :\n",
    "\n",
    "-   Fine-tuned BERT outperforms all other models in test accuracy, achieving 91.4%, which is about +3% better than the LSTM-based models and the custom transformer.\n",
    "\n",
    "-   Test loss for BERT is higher (0.3739) than the custom transformer and LSTM models, meaning BERT’s predictions are slightly less confident (loss measures how sharp and certain predictions are).\n",
    "\n",
    "-   Despite a higher loss, BERT generalizes better, making more correct predictions overall."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f27045",
   "metadata": {},
   "source": [
    "Overall, the fine-tuned model achieved the highest test accuracy (91.4%) compared to all other models, outperforming both LSTM-based and custom Transformer models by a margin of about 3%. Despite having a slightly higher test loss, BERT’s superior performance can be justified by its deep contextual understanding of language, learned from massive pretraining on large text corpora. This enables BERT to generalize better on unseen data, making it overall the most effective model for the sentiment classification task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
